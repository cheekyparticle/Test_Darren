#! /usr/bin/env python

"""\
%prog <runsdir> [<ipolfile>=ipol.dat] [opts]

Interpolate histo bin values as a function of the parameter space by loading
from the usual data directory structure $datadir/mc/{rundirs}

TODO:
 * Use weight file position matches to exclude some bins, as well as path matching
 * Handle run combination file/string (write a hash of the run list into the ipol filename?)
 * Support asymm error parameterisation
"""

import optparse, os, sys
op = optparse.OptionParser(usage=__doc__)
op.add_option("--pname", "--pfile", dest="PNAME", default="params.dat", help="Name of the params file to be found in each run directory (default: %default)")
op.add_option("--wfile", dest="WFILE", default=None, help="Path to a weight file, used to restrict ipol building to a subset of bins (default: %default)")
op.add_option("--ierr",  dest="IERR", default="median", help="Whether to interpolate MC errors: none, mean, median, symm (default: %default)") #< add rel, asymm
op.add_option("--rc", dest="RUNCOMBS", default=None, help="Run combination file")
# TODO: Add a no-scale option
# TODO: Change to instead (optionally) specify the max number of parallel threads/procs. Use by default if ncores > 1?
op.add_option("-j", dest="MULTI", type=int, default=1, help="Number of threads to use")
op.add_option("--summ",  dest="SUMMARY", default=None, help="Summary description to be written to the ipol output file")
op.add_option("-v", "--debug", dest="DEBUG", action="store_true", default=False, help="Turn on some debug messages")
op.add_option("-q", "--quiet", dest="QUIET", action="store_true", default=False, help="Turn off messages")
op.add_option("--mlc", dest="MLC", type=float, default=1.0, help="The C parameter in the SVR")
op.add_option("--mlepsilon", dest="MLEPSILON", type=float, default=0.0, help="The epsilon parameter in the SVR")
opts, args = op.parse_args()

## Get mandatory arguments
if len(args) < 1:
    print "Argument missing... exiting\n\n"
    op.print_usage()
    sys.exit(1)
RUNSDIR = args[0]
IFILE = "ml.pkl"
if len(args) >= 2:
    IFILE = args[1]


## Load the Professor machinery
import professor2 as prof
if not opts.QUIET:
    print prof.logo


## Load MC run histos and params
import glob
INDIRS = glob.glob(os.path.join(RUNSDIR, "*"))
try:
    PARAMS, HISTOS = prof.read_rundata(INDIRS, opts.PNAME)
    RUNS, PARAMNAMES, PARAMSLIST = prof.mk_ipolinputs(PARAMS)
except Exception, e:
    print e
    sys.exit(1)

# TODO: add runcomb file parsing to select a runs subset
# --rc runcombs.dat:4
# would use the 4th line of the runcombs.dat file
if opts.RUNCOMBS is not None:
    f_rc, line = opts.RUNCOMBS.split(":")
    with open(f_rc) as f:
        temp=[l for l in f]
    thisRC = temp[int(line)].split()

    # Filtering and overwriting
    thisRUNS, thisPARAMSLIST = [], []
    for num, r in enumerate(RUNS):
        if r in thisRC:
            thisRUNS.append(r)
            thisPARAMSLIST.append(PARAMSLIST[num])
    RUNS=thisRUNS
    PARAMSLIST=thisPARAMSLIST


# ## Some useful announcements about the data loaded and the interpolation planned
# if not opts.QUIET:
    # if (len(RUNS) < prof.numCoeffs(len(PARAMNAMES), opts.ORDER)):
        # print "Not enough runs for order %i polynomials"%opts.ORDER
        # for i in xrange(1, opts.ORDER):
            # if (prof.numCoeffs(len(PARAMNAMES), opts.ORDER -i) <= len(RUNS)):
                # print "Try order %i (min %i runs)"%(opts.ORDER -i, prof.numCoeffs(len(PARAMNAMES), opts.ORDER -i))
        # import sys
        # sys.exit(1)
    # else:
        # print "Building %dD interpolations in %d params: require at least %d runs" % \
            # (opts.ORDER, len(PARAMNAMES), prof.numCoeffs(len(PARAMNAMES), opts.ORDER))
        # print "Loaded %d distinct observables from %d runs" % (len(HISTOS), len(RUNS))


## Weight file parsing to select a histos subset
if opts.WFILE:
    matchers = prof.read_pointmatchers(opts.WFILE)
    for hn in HISTOS.keys():
        if not any(m.match_path(hn) for m in matchers.keys()):
            del HISTOS[hn]
        elif opts.DEBUG:
            print "Observable %s passed weight file path filter" % hn
    print "Filtered observables by path, %d remaining" % len(HISTOS)
HNAMES = HISTOS.keys()

## If there's nothing left to interpolate, exit!
if not HNAMES:
    print "No observables remaining... exiting"
    sys.exit(1)


## Robustness tests and cleaning: only retain runs that contain every histo
# TODO: combine with weights histo vetoing -- should we explicitly unload unused run data, or keep it for the next combination to use? Or do we now leave runcombs to the user?
bad, badnum = [], []
for irun, run in enumerate(RUNS):
    for hn in HNAMES:
        if not HISTOS[hn].has_key(run):
            bad.append(run)
            badnum.append(irun)
            break
if bad:
    print "Found %d bad runs in %d total... removing" % (len(bad), len(RUNS))
    goodr, goodp = [], []
    for irun, run in enumerate(RUNS):
        if not irun in badnum:
            goodr.append(run)
            goodp.append(PARAMSLIST[irun])
    RUNS = goodr
    PARAMSLIST = goodp

## If there's nothing left to interpolate, exit!
if not RUNS:
    print "No valid runs remaining... exiting"
    sys.exit(1)


IHISTOS = {}

from professor2.ml import *

def worker(q, rdict):
    "Function to make bin ipols and store ipol persistency strings for each histo"
    while True:
        if q.empty():
            break
        hn = q.get()
        histos = HISTOS[hn]
        ih = mk_MLHisto(histos, RUNS, PARAMSLIST, pC=opts.MLC, pEps=opts.MLEPSILON)
        del HISTOS[hn] #< pro-actively clear up memory
        rdict[hn] = ih#zlib.compress(s, 9) #< save some memory
        del histos


print "\n\nParametrising...\n"
import time, multiprocessing
time1 = time.time()

## A shared memory object is required for coefficient retrieval
from multiprocessing import Manager
manager = Manager()
tempDict = manager.dict()

## The job queue
q = multiprocessing.Queue()
map(lambda x:q.put(x), HNAMES)

## Fire away
workers = [multiprocessing.Process(target=worker, args=(q, tempDict)) for i in range(opts.MULTI)]
map(lambda x:x.start(), workers)
map(lambda x:x.join(),  workers)

# ## Finally copy the result dictionary into the object itself
for k in tempDict.keys():
    IHISTOS[k] = tempDict[k]

# for HN in HNAMES:
    # histos = HISTOS[HN]
    # IHISTOS[HN] = mk_MLHisto(histos, RUNS, PARAMSLIST)

## Timing
time2 = time.time()
print 'Parametrisation took %0.2f s' % ((time2-time1))



## Active memory clean-up
del HISTOS

import cPickle
cPickle.dump(IHISTOS, open(IFILE, 'wb'), -1)
print "\nOutput written to %s" % IFILE

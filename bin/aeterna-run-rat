#!/usr/bin/env python

# from __future__ import absolute_import, unicode_literals, print_function
import matplotlib, os, sys
matplotlib.use(os.environ.get("MPL_BACKEND", "Agg"))
import pymultinest
import mpi4py
# A bit of a hack to compactly have the script work with and without mpi
rank = 0
try:
    from mpi4py import MPI
    comm = MPI.COMM_WORLD
    size = comm.Get_size()
    rank = comm.Get_rank()
except:
    pass
# A bit of a hack to compactly have the script work with and without mpi
rank = 0
try:
    from mpi4py import MPI
    comm = MPI.COMM_WORLD
    size = comm.Get_size()
    rank = comm.Get_rank()
except:
    pass


__doc__="""
     %s [OPTIONS] -r REFDIR   <ipolfiles>
"""%(sys.argv[0])

class SignalGenerator(object):
    """
    Class for single patch.
    """
    def __init__(self, ifile, expression=None, extraparams=[], fixparams=[], ipolconf={}):
        self._ifile = ifile
        self.load()

    def load(self):
        import pyrapp
        import json
        d = json.load(open(self._ifile))
        use = [str(k) for k in d.keys() if "counts" in k]
        use.sort(key=lambda x: int(x.split("#")[-1]))
        self._rapp = [pyrapp.Rapp(d[x]) for x in use]

    def __call__(self, x):
        import numpy as np
        return np.exp([r(x) for r in self._rapp])


# Import some prof stuff
import optparse, os, sys
op = optparse.OptionParser(usage=__doc__)
op.add_option("--output", dest="OUTPUT", default="chains", type=str, help="Prefix for outputs (default: %default)")
op.add_option("--tol", dest="TOL", default=0.1, type=float, help="Evidence tolerance (default: %default)")
op.add_option("--eff", dest="EFF", default=0.8, type=float, help="Sampling efficiency (default: %default)")
op.add_option("--points", dest="POINTS", default=1000, type=int, help="Number of live points in PyMultinest (default: %default)")
op.add_option("--resume", dest="RESUME", default=False, action='store_true', help="Resume on previous run.")
op.add_option("--update", dest="UPDATE", default=10000, type=int, help="Update inteval (default: %default iterations)")
op.add_option("-d", "--datadir", dest="DATADIR", default=None, help="The data directory")
op.add_option("-v", "--debug", dest="DEBUG", action="store_true", default=False, help="Turn on some debug messages")
op.add_option("-q", "--quiet", dest="QUIET", action="store_true", default=False, help="Turn off messages")
opts, args = op.parse_args()

## Get mandatory arguments
if len(args) < 1:
    print "Argument missing... exiting\n\n"
    op.print_usage()
    sys.exit(1)

## Get mandatory options
if opts.DATADIR is None:
    print "No datadir specified... exiting\n\n"
    op.print_usage()
    sys.exit(1)

# The data thingie
import professor2 as prof
DHISTOS = prof.read_all_histos(opts.DATADIR)
if len(DHISTOS.keys())==0:
    print "Error, could not load any yoda files from folder", opts.DATADIR
    exit(1)

# Test if requested ipol files actually exist
for a in args:
    if not os.path.exists(a):
        print "Error, ipol file %s does not exist"%a
        sys.exit(1)

S = SignalGenerator(args[0])
PMIN = S._rapp[0]._scaler._Xmin
PMAX = S._rapp[0]._scaler._Xmax
PLEN=[PMAX[i] - PMIN[i] for i in xrange(len(PMAX))]

## Prepare lists of ibins and dbins
from scipy.special import gamma
from math import exp, log
DBINS, DVALS, log_DGAMMA = {}, {}, {}
available=["/DM/XENON/counts"]
for a in available:
    DBINS[a]=[]
    DVALS[a]=[]
    log_DGAMMA[a]=[]
    for nb in xrange(len(DHISTOS[a].bins)):
        data    = DHISTOS[a].bins[nb].val # log
        dataerr = DHISTOS[a].bins[nb].err # log
        if dataerr <0:
            DVALS[a].append(0.0)
            log_DGAMMA[a].append(0.0)
        else:
            DBINS[a].append(DHISTOS[a].bins[nb]) # log
            DVALS[a].append(exp(data)) # non log
            if DVALS[a][-1] < 50: #"Use gamm when smaller than 50"
                log_DGAMMA[a].append(log(gamma(DVALS[a][-1]+1))) # Needs understanding
            else:
                log_DGAMMA[a].append(DVALS[a][-1] * data - DVALS[a][-1])

    if rank==0: print "Using %i bins for %s"%(len(DVALS[a]), a)


def scaleParam(p, idx):
    return PMIN[idx] + p * PLEN[idx]


# The prior is nothing else but turning coordinates from
# a [0,1] hypercube into our parameter space hypercube
# It is passed to pymultinest.run
def myprior(cube, ndim, nparams):
    for i in range(ndim):
        cube[i] = scaleParam(cube[i], i)


from numpy import log, exp
def myloglike(cube, ndim, nparams):
    N_ipol = S([cube[0], cube[1]])
    log_N_ipol = log(N_ipol)
    LoLi=0
    for datakey in available:
        for i in xrange(len(DVALS[datakey])):
            N_data = DVALS[datakey][i]
            LoLi += N_data * log_N_ipol[i] - log_DGAMMA[datakey][i]
            LoLi -= N_ipol[i]
    return LoLi


# Create output directory
if not os.path.exists(opts.OUTPUT): os.mkdir(opts.OUTPUT)

# Number of dimensions our problem has
n_params = len(PMIN)


# Run MultiNest
pymultinest.run(myloglike, myprior, n_params, importance_nested_sampling = True, verbose = False,
        multimodal=False, resume=opts.RESUME, n_iter_before_update=opts.UPDATE,
        evidence_tolerance=opts.TOL, sampling_efficiency = opts.EFF,
        n_live_points = opts.POINTS,
        outputfiles_basename='%s/AETERNA'%opts.OUTPUT, init_MPI=False)


if rank==0:

    # lets analyse the results
    print()
    print("Now analyzing output")
    a = pymultinest.Analyzer(n_params = n_params, outputfiles_basename='%s/AETERNA'%opts.OUTPUT)
    s = a.get_stats()

    import json
    with open('%sstats.json' % a.outputfiles_basename, mode='w') as f:
            json.dump(s, f, indent=2)
    print()
    print("-" * 30, 'ANALYSIS', "-" * 30)
    print("Global Evidence:\n\t%.15e +- %.15e" % ( s['nested sampling global log-evidence'], s['nested sampling global log-evidence error'] ))

    print("Done!")
    import sys
    sys.exit(1)
